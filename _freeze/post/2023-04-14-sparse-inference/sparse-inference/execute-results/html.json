{
  "hash": "83f360e95f6292db190a5dd3b564a6c5",
  "result": {
    "markdown": "---\ntitle: \"Sparse Inference\"\ndescription: |\n  Examining sparse inference using both frequentist and bayesian methodology.\nbibliography: references.bib\nauthor:\n  - name: \"Richard Aubrey White\"\n    url: https://www.rwhite.no\n    orcid: 0000-0002-6747-1726\ndate: 2023-04-14\ntitle-block-banner: \"#b8e2f2\"\neditor_options: \n  chunk_output_type: console\n---\n\n\n## Introduction\n\nMachine learning and statistical modeling are two important tools in data science that are used to make predictions and infer relationships between variables, respectively. Models for prediction aim to estimate the relationship between inputs and outputs in order to make accurate predictions about new, unseen data. In contrast, models for inference aim to understand the underlying relationships between variables in the data, often in the form of identifying causal relationships.\n\nIn this blog post, we will explore using models for inference using a simulated dataset, and we will apply penalized regression to perform feature selection on a binary outcome. Penalized regression is particularly useful in situations where the number of predictors (i.e. independent variables) is much larger than the sample size.\n\nWe will investigate the frequentist solution of using a two-stage solution with LASSO regression via `glmnet` and then using the `selectiveInference` package to perform post inference and adjust for the bias introduced by the selection process. We will also investigate a Bayesian solution that approximates a LASSO regression via a Laplace prior.\n\n## Simulating a Dataset\n\nWe will simulate a dataset with `n = 5000` people and `p = 50` variables, where only three of the 50 variables will have an association with the binary outcome, and they will have odds ratios of 4, 3, and 2. The remaining variables will have no association with the outcome.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(data.table)\nlibrary(magrittr)\nlibrary(ggplot2)\n\nset.seed(123)\nn <- 5000\np <- 50\nx <- matrix(rnorm(n * p), nrow = n)\nbeta <- c(log(4), log(3), log(2), rep(0, 47))\nprob <- plogis(x %*% beta)\ny <- rbinom(n, 1, prob)\n\ndata <- data.frame(cbind(y,x))\ncolnames(data) <- c(\"y\", paste0(\"x\",1:ncol(x)))\n\nx <- model.matrix(y ~ ., data = data)[,-1]\ny <- data$y\n```\n:::\n\n\n\n## LASSO Regression\n\nWe will now fit a LASSO regression model using the `glmnet` package in R. LASSO is a popular method for feature selection in high-dimensional data, where the number of predictors `p` is much larger than the number of observations `n`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# get standard deviation of X, because we will need to standardize/scale it outside of glmnet\nsds <- apply(x, 2, sd)\n\n# standardize x\nx_scaled <- scale(x,TRUE,TRUE)\n\n# run glmnet\ncfit <- glmnet::cv.glmnet(x_scaled,y,standardize=FALSE, family=\"binomial\")\ncoef(cfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n51 x 1 sparse Matrix of class \"dgCMatrix\"\n                     s1\n(Intercept) 0.008303975\nx1          1.081745656\nx2          0.821477348\nx3          0.452041341\nx4          .          \nx5          .          \nx6          .          \nx7          .          \nx8          .          \nx9          .          \nx10         .          \nx11         .          \nx12         .          \nx13         .          \nx14         .          \nx15         .          \nx16         .          \nx17         .          \nx18         .          \nx19         .          \nx20         .          \nx21         .          \nx22         .          \nx23         .          \nx24         .          \nx25         .          \nx26         .          \nx27         .          \nx28         .          \nx29         .          \nx30         .          \nx31         .          \nx32         .          \nx33         .          \nx34         .          \nx35         .          \nx36         .          \nx37         .          \nx38         .          \nx39         .          \nx40         .          \nx41         .          \nx42         .          \nx43         .          \nx44         .          \nx45         .          \nx46         .          \nx47         .          \nx48         .          \nx49         .          \nx50         .          \n```\n:::\n:::\n\n\n::: {.callout-info}\n\n## No confidence intervals\n\nNote that LASSO regression does not provide any confidence intervals or p-values, only coeficient estimates.\n\n:::\n\n### Inference\n\nLASSO regression is a popular method for variable selection in high-dimensional datasets. It shrinks some coefficients to zero, allowing us to select only a subset of variables that have the strongest association with the outcome. However, LASSO does not provide confidence intervals or p-values for the selected variables.\n\nThe reason for this is that LASSO performs variable selection by penalizing the likelihood function, not by explicitly testing the significance of each variable. Therefore, we cannot use traditional methods to compute confidence intervals or p-values. Instead, we need to use methods that are specifically designed for post-selection inference.\n\nOne such method is provided in the R package `selectiveInference.` The function `fixedLassoInf` provides confidence intervals and p-values for LASSO selected variables by accounting for the fact that variable selection was performed. It does this by using a two-stage procedure. In the first stage, LASSO selects a subset of variables. In the second stage, selectiveInference performs inference on the selected variables, adjusting for the selection procedure.\n\n::: {.callout-warning}\n\n## Remember to account for decisions taken at all stages of your modelling process.\n\nIt is important to use `selectiveInference` rather than naively using simpler (but incorrect) methods that do not take into account the two-stage process. \n\nA simpler (but incorrect) method involves first selecting variables with LASSO and then fitting a traditional logistic regression on the selected variables. However, this can lead to biased estimates because the LASSO selection process ignores the uncertainty in the variable selection. Therefore, the second-stage regression will not account for the fact that the variable selection was performed, leading to over-optimistic estimates of the significance of the selected variables.\n\nBy using `selectiveInference`, we can properly account for the selection process and obtain unbiased estimates of the significance of the selected variables.\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compute fixed lambda p-values and selection intervals\nout <- selectiveInference::fixedLassoInf(\n  x = x_scaled,\n  y = y,\n  beta = coef(cfit),\n  lambda = cfit$lambda.1se,\n  alpha = 0.05,\n  family = \"binomial\"\n)\n\nretval <- data.frame(\n  var = names(out$vars),\n  Odds_Ratio = exp(out$coef0/sds[out$vars]),\n  LowConf = exp(out$ci[,1]/sds[out$vars]),\n  UpperConf = exp(out$ci[,2]/sds[out$vars]),\n  pval = out$pv\n)\nrow.names(retval) <- NULL\nretval$var[retval$pval < 0.05] <- paste0(\"*\", retval$var[retval$pval < 0.05])\nnames(retval) <- c(\n  \"Variable\",\n  \"Odds ratio\",\n  \"Conf int 5%\",\n  \"Conf int 95%\",\n  \"Pvalue\"\n)\nretval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Variable Odds ratio Conf int 5% Conf int 95% Pvalue\n1      *x1   3.761571    3.471404     4.073621      0\n2      *x2   2.793509    2.594666     3.007784      0\n3      *x3   1.848946    1.727840     1.979062      0\n```\n:::\n:::\n\n\n## Bayesian Logistic Regression using `rstanarm`\n\nAnother way to perform inference on a logistic regression model with feature selection is through Bayesian methods. In particular, we can use the `rstanarm` R package to fit a Bayesian logistic regression model with a Laplace prior. \n\n::: {.callout-info}\n\n## Laplace prior\n\nThe Laplace prior is used to promote sparsity by assigning a probability distribution to the coefficients that puts more probability mass around zero. It is equivalent to LASSO regression [@Tibshirani1996].\n\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\noptions(mc.cores = parallel::detectCores())\n\nfit <- rstanarm::stan_glm(\n  formula = y ~ .,\n  data = data,\n  family = binomial(),\n  prior = rstanarm::laplace(),\n  chains = 4,\n  iter = 5000,\n  refresh=0\n)\n\nretval <- data.frame(\n  var = names(coef(fit)),\n  Odds_Ratio = round(exp(coef(fit)),3),\n  round(exp(rstanarm::posterior_interval(fit, prob = 0.9)),3),\n  pvalue_equivalent = round(bayestestR::pd_to_p(bayestestR::p_direction(fit)$pd),2)\n)\nrow.names(retval) <- NULL\nretval$var[retval$pvalue_equivalent < 0.05] <- paste0(\"*\", retval$var[retval$pvalue_equivalent < 0.05])\nnames(retval) <- c(\n  \"Variable\",\n  \"Odds ratio\",\n  \"Cred int 5%\",\n  \"Cred int 95%\",\n  \"Pvalue equivalent\"\n)\nretval\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      Variable Odds ratio Cred int 5% Cred int 95% Pvalue equivalent\n1  (Intercept)      1.009       0.951        1.070              0.80\n2          *x1      4.047       3.742        4.387              0.00\n3          *x2      2.969       2.768        3.186              0.00\n4          *x3      1.932       1.817        2.061              0.00\n5           x4      0.953       0.897        1.011              0.17\n6           x5      1.018       0.960        1.078              0.62\n7           x6      0.999       0.941        1.060              0.98\n8           x7      0.984       0.927        1.045              0.64\n9           x8      0.974       0.917        1.034              0.46\n10          x9      0.960       0.904        1.019              0.26\n11         x10      0.998       0.940        1.059              0.95\n12         x11      1.042       0.982        1.105              0.26\n13         x12      0.962       0.907        1.021              0.28\n14         x13      1.040       0.979        1.103              0.29\n15         x14      1.002       0.943        1.067              0.95\n16         x15      0.999       0.942        1.060              0.98\n17         x16      0.961       0.904        1.017              0.26\n18         x17      1.032       0.973        1.094              0.38\n19         x18      0.957       0.901        1.016              0.22\n20         x19      0.993       0.936        1.055              0.85\n21         x20      1.006       0.947        1.069              0.86\n22         x21      0.984       0.928        1.043              0.66\n23         x22      1.043       0.984        1.106              0.24\n24         x23      0.958       0.902        1.017              0.24\n25         x24      0.997       0.938        1.059              0.93\n26         x25      1.040       0.980        1.104              0.27\n27         x26      0.980       0.924        1.038              0.58\n28         x27      1.038       0.978        1.101              0.31\n29         x28      1.012       0.956        1.072              0.72\n30         x29      1.023       0.965        1.086              0.52\n31         x30      1.008       0.950        1.068              0.82\n32        *x31      0.903       0.851        0.960              0.00\n33         x32      1.025       0.969        1.087              0.47\n34         x33      1.032       0.971        1.096              0.38\n35         x34      1.002       0.944        1.062              0.97\n36         x35      1.007       0.951        1.066              0.84\n37         x36      1.049       0.989        1.110              0.18\n38         x37      1.023       0.964        1.085              0.51\n39         x38      1.060       0.999        1.125              0.11\n40         x39      1.021       0.963        1.084              0.56\n41         x40      1.030       0.971        1.092              0.43\n42         x41      0.985       0.927        1.045              0.68\n43         x42      1.005       0.947        1.068              0.89\n44         x43      1.025       0.966        1.089              0.50\n45         x44      0.947       0.891        1.003              0.12\n46         x45      1.044       0.984        1.108              0.24\n47         x46      1.012       0.953        1.072              0.74\n48         x47      1.006       0.948        1.065              0.88\n49         x48      0.953       0.900        1.012              0.20\n50         x49      0.960       0.906        1.019              0.27\n51         x50      0.968       0.913        1.028              0.39\n```\n:::\n:::\n\n\n::: {.callout-info}\n\n## P-value equivalent\n\nProbability of Direction (PoD) and p-values are both statistical measures used in hypothesis testing [@Makowski2019bayestestR, @Makowski2019Indices]. They are similar in that they both provide evidence for or against a null hypothesis.\nPoD measures the proportion of posterior draws from a Bayesian model that are in the direction of the alternative hypothesis. It provides a measure of the strength of evidence for the alternative hypothesis relative to the null hypothesis. A high PoD value indicates strong evidence in favor of the alternative hypothesis, while a low PoD value indicates weak evidence in favor of the alternative hypothesis.\n\nSimilarly, a p-value measures the probability of obtaining a test statistic as extreme as or more extreme than the observed value, assuming that the null hypothesis is true. A low p-value indicates that the observed result is unlikely to have occurred by chance alone, providing evidence against the null hypothesis.\n\nTo convert PoD to a p-value equivalent, one approach is to use the following formula:\n\np-value = 2 * min(PoD, 1-PoD)\n\nThis formula assumes a two-tailed test and converts the PoD to a p-value for a test of the null hypothesis that the effect size is equal to zero. The resulting p-value can be interpreted as the probability of obtaining the observed result or a more extreme result under the null hypothesis.\n\n:::\n\n## Conclusion\n\nThe blog article discusses the limitations of using LASSO (Least Absolute Shrinkage and Selection Operator) models for statistical inference, particularly in situations where the number of predictors (i.e. independent variables) is much larger than the sample size. In these cases, LASSO models can suffer from high variability in the estimated coefficients, which can lead to incorrect or unreliable conclusions.\n\nOne proposed solution to this problem is to use a two-stage inference approach, where LASSO is first used to select a subset of predictors, and then a separate statistical method (such as ordinary least squares) is used to estimate the coefficients for the selected predictors. However, this two-stage approach can also have limitations, such as a loss of power in the second stage and increased computational complexity.\n\nIn contrast, Bayesian statistics offer a one-stage inference approach that can provide more reliable and interpretable results in complex modeling situations. Bayesian statistics allow for the incorporation of prior knowledge and uncertainty in the model, which can help to reduce variability and improve accuracy. Bayesian methods also provide a framework for model comparison and selection, which can help to identify the most appropriate model for a given dataset.\n\nOverall, while LASSO models can be useful in certain situations, their limitations in high-dimensional data settings highlight the advantages of Bayesian statistics for reliable and interpretable statistical inference.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}